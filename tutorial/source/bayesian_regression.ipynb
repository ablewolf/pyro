{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Regression - Introduction (Part 1)\n",
    "\n",
    "Regression is one of the most common and basic supervised learning tasks in machine learning. Suppose we're given a dataset $\\mathcal{D}$ of the form\n",
    "\n",
    "$$ \\mathcal{D}  = \\{ (X_i, y_i) \\} \\qquad \\text{for}\\qquad i=1,2,...,N$$\n",
    "\n",
    "The goal of linear regression is to fit a function to the data of the form:\n",
    "\n",
    "$$ y = w X + b + \\epsilon $$\n",
    "\n",
    "where $w$ and $b$ are learnable parameters and $\\epsilon$ represents observation noise. Specifically $w$ is a matrix of weights and $b$ is a bias vector.\n",
    "\n",
    "Let's first implement linear regression in PyTorch and learn point estimates for the parameters $w$ and $b$.  Then we'll see how to incorporate uncertainty into our estimates by using Pyro to implement Bayesian linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's begin by importing the modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.distributions.util import logsumexp\n",
    "from pyro.infer import EmpiricalMarginal, SVI, Trace_ELBO, TracePredictive\n",
    "from pyro.infer.mcmc import MCMC, NUTS\n",
    "import pyro.optim as optim\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "# for CI testing\n",
    "smoke_test = ('CI' in os.environ)\n",
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(1)\n",
    "DATA_URL = \"https://d2fefpcigoriu7.cloudfront.net/datasets/rugged_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset \n",
    "\n",
    "We would like to explore the relationship between topographic heterogeneity of a nation as measured by the Terrain Ruggedness Index (variable *rugged* in the dataset) and its GDP per capita. In particular, it was noted by the authors in \\[1\\] that terrain ruggedness or bad geography is related to poorer economic performance outside of Africa, but rugged terrains have had a reverse effect on income for African nations. Let us look at the dataset and plot this relationship.\n",
    "\n",
    "We will be focusing on three columns from the dataset:\n",
    "  - `rugged`: quantifies the Terrain Ruggedness Index\n",
    "  - `cont_africa`: whether the given nation is in Africa\n",
    "  - `rgdppc_2000`: Real GDP per capita for the year 2000\n",
    "  \n",
    "We will also take the logarithm for the response variable, GDP. This is a common practice as the relationship of wealth to its causal factors is often exponential due to the *rich get richer phenomenon*.\n",
    "\n",
    "[1] Nunn, N. & Puga, D., *[Ruggedness: The blessing of bad geography in Africa\"](https://diegopuga.org/papers/rugged.pdf)*, Review of Economics and Statistics 94(1), Feb. 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\n",
    "df = data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "df = df[np.isfinite(df.rgdppc_2000)]\n",
    "df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "african_nations = data[data[\"cont_africa\"] == 1]\n",
    "non_african_nations = data[data[\"cont_africa\"] == 0]\n",
    "sns.regplot(non_african_nations[\"rugged\"], \n",
    "            np.log(non_african_nations[\"rgdppc_2000\"]), \n",
    "            ax=ax[0])\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"Non African Nations\")\n",
    "sns.regplot(african_nations[\"rugged\"], \n",
    "            np.log(african_nations[\"rgdppc_2000\"]), \n",
    "            ax=ax[1])\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"African Nations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "We would like to predict log GDP per capita of a nation as a function of two features from the dataset - whether the nation is in Africa, and its Terrain Ruggedness Index.  Let's define our regression model. We'll use PyTorch's `nn.Module` for this.  Our input $X$ is a matrix of size $N \\times p$ and our output $y$ is a vector of size $p \\times 1$.  The function `nn.Linear(p, 1)` defines a linear transformation of the form $Xw + b$ where $w$ is the weight matrix and $b$ is the additive bias.  As you can see, we can easily make this a logistic regression by adding a non-linearity in the `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        # p = number of features\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(p, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "p = 2  # number of features\n",
    "regression_model = RegressionModel(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We will use the mean squared error (MSE) as our loss and Adam as our optimizer. We would like to optimize the parameters of the `regression_model` neural net above. We will use a somewhat large learning rate of `0.01` and run for 500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optim = torch.optim.Adam(regression_model.parameters(), lr=0.05)\n",
    "num_iterations = 1000 if not smoke_test else 2\n",
    "data = torch.tensor(df.values, dtype=torch.float)\n",
    "\n",
    "def main():\n",
    "    x_data = data[:, :-1]\n",
    "    y_data = data[:, -1]\n",
    "    for j in range(num_iterations):\n",
    "        # run the model forward on the data\n",
    "        y_pred = regression_model(x_data).squeeze(-1)\n",
    "        # calculate the mse loss\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "        # initialize gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        if (j + 1) % 50 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "    # Inspect learned parameters\n",
    "    print(\"Learned parameters:\")\n",
    "    for name, param in regression_model.named_parameters():\n",
    "        print(name, param.data.numpy())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "[Bayesian modeling](http://mlg.eng.cam.ac.uk/zoubin/papers/NatureReprint15.pdf) offers a systematic framework for reasoning about model uncertainty. Instead of just learning point estimates, we're going to learn a _distribution_ over values of the parameters $w$ and $b$ that are consistent with the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Regression\n",
    "\n",
    "In order to make our linear regression Bayesian, we need to put priors on the parameters $w$ and $b$. These are distributions that represent our prior belief about reasonable values for $w$ and $b$ (before observing any data).\n",
    "\n",
    "### `random_module()`\n",
    "\n",
    "In order to do this, we'll 'lift' the parameters $w$ and $b$ to random variables. We can do this in Pyro via `random_module()`, which effectively takes a given `nn.Module` and turns it into a distribution over the same module; in our case, this will be a distribution over regressors. Specifically, each parameter in the original regression model is sampled from the provided prior. This allows us to repurpose vanilla regression models for use in the Bayesian setting. For example:\n",
    "```python\n",
    "loc = torch.zeros(1, 1)\n",
    "scale = torch.ones(1, 1)\n",
    "# define a unit normal prior\n",
    "prior = Normal(loc, scale)\n",
    "# overload the parameters in the regression module with samples from the prior\n",
    "lifted_module = pyro.random_module(\"regression_module\", regression_model, prior)\n",
    "# sample a regressor from the prior\n",
    "sampled_reg_model = lifted_module()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We now have all the ingredients needed to specify our model. First we define priors over $w$ and $b$. Note the priors that we are using for the different latent variables in the model. The prior on the intercept parameter is very flat as we would like this to be learnt from the data. We are using a weakly regularizing prior on the regression coefficients to avoid overfitting to the data.  We wrap `regression_model` with `random_module` and sample an instance of the regressor, `lifted_reg_model`. We then run the regressor forward on the inputs `x_data`. Finally we use the `obs` argument to the `pyro.sample` statement to condition on the observed data `y_data` with a learned observation noise `sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    w_prior = Normal(torch.zeros(1, 2), torch.ones(1, 2)).independent(1)\n",
    "    b_prior = Normal(torch.tensor([[8.]]), torch.tensor([[1000.]])).independent(1)\n",
    "    priors = {'linear.weight': w_prior, 'linear.bias': b_prior}\n",
    "    scale = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, priors)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    with pyro.plate(\"map\", len(data)):\n",
    "        x_data = data[:, :-1]\n",
    "        y_data = data[:, -1]\n",
    "    \n",
    "        # run the regressor forward conditioned on data\n",
    "        prediction_mean = lifted_reg_model(x_data).squeeze(-1)\n",
    "        # condition on the observed data\n",
    "        pyro.sample(\"obs\", \n",
    "                    Normal(prediction_mean, scale),\n",
    "                    obs=y_data)\n",
    "        return prediction_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide\n",
    "\n",
    "In order to do inference we're going to need a guide, i.e. a variational family of distributions.  We will use Pyro's [autoguide library](http://docs.pyro.ai/en/dev/contrib.autoguide.html) to automatically place Gaussians with diagonal covariance on all of the distributions in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we choose Gaussians for both guide distributions. Also, to ensure positivity, we pass each log scale through a `softplus()` transformation (an alternative to ensure positivity would be an `exp()`-transformation).\n",
    "\n",
    "## Inference\n",
    "\n",
    "To do inference we'll use stochastic variational inference (SVI) (for an introduction to SVI, see [SVI Part I](svi_part_i.ipynb)). Just like in the non-Bayesian linear regression, each iteration of our training loop will take a gradient step, with the difference that in this case, we'll use the ELBO objective instead of the MSE loss by constructing a `Trace_ELBO` object that we pass to `SVI`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.05})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO(), num_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `Adam` is a thin wrapper around `torch.optim.Adam` (see [here](svi_part_i.ipynb#Optimizers) for a discussion). The complete training loop is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    pyro.clear_param_store()\n",
    "    for j in range(num_iterations):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss = svi.step(data)\n",
    "        if j % 100 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(data)))\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take an ELBO gradient step we simply call the `step` method of `SVI`. Notice that the `data` argument we pass to `step` will be passed to both `model()` and `guide()`.\n",
    "\n",
    "## Validating Results\n",
    "Let's compare the variational parameters we learned to our previous result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, instead of just point estimates, we now have uncertainty estimates (`auto_scale`) for our learned parameters.  Note that Autoguide packs the latent variables into a tensor, in this case, one entry per variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "To evaluate our model, we'll generate some predictive samples and look at the posteriors. Since our variational distribution is fully parameterized, we can just run the lifted model forward.  We wrap the model with a `Delta` distribution in order to register the values with the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped_model(data):\n",
    "    pyro.sample(\"prediction\", dist.Delta(model(data)))\n",
    "\n",
    "posterior = svi.run(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def summary(traces, sites):\n",
    "    marginal = EmpiricalMarginal(traces, sites).get_samples_and_weights()[0].detach().cpu().numpy()\n",
    "    site_stats = {}\n",
    "    for i in range(marginal.shape[1]):\n",
    "        site_name = sites[i]\n",
    "        marginal_site = pd.DataFrame(marginal[:, i]).transpose()\n",
    "        describe = partial(pd.Series.describe, percentiles=[.05, 0.25, 0.5, 0.75, 0.95])\n",
    "        site_stats[site_name] = marginal_site.apply(describe, axis=1) \\\n",
    "            [[\"mean\", \"std\", \"5%\", \"25%\", \"50%\", \"75%\", \"95%\"]]\n",
    "    return site_stats\n",
    "\n",
    "trace_pred = TracePredictive(wrapped_model,\n",
    "                             posterior,\n",
    "                             num_samples=1000)\n",
    "post_pred = trace_pred.run(data)\n",
    "post_summary = summary(post_pred, sites= ['prediction', 'obs'])\n",
    "mu = post_summary[\"prediction\"]\n",
    "y = post_summary[\"obs\"]\n",
    "predictions = pd.DataFrame({\n",
    "    \"cont_africa\": data[:, 0],\n",
    "    \"rugged\": data[:, 1],\n",
    "    \"mu_mean\": mu[\"mean\"],\n",
    "    \"mu_perc_5\": mu[\"5%\"],\n",
    "    \"mu_perc_95\": mu[\"95%\"],\n",
    "    \"y_mean\": y[\"mean\"],\n",
    "    \"y_perc_5\": y[\"5%\"],\n",
    "    \"y_perc_95\": y[\"95%\"],\n",
    "    \"true_gdp\": data[:, 2]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "african_nations = predictions[predictions[\"cont_africa\"] == 1]\n",
    "non_african_nations = predictions[predictions[\"cont_africa\"] == 0]\n",
    "african_nations = african_nations.sort_values(by=[\"rugged\"])\n",
    "non_african_nations = non_african_nations.sort_values(by=[\"rugged\"])\n",
    "fig.suptitle(\"Regression line 90% CI\", fontsize=16)\n",
    "ax[0].plot(non_african_nations[\"rugged\"],\n",
    "           non_african_nations[\"mu_mean\"])\n",
    "ax[0].fill_between(non_african_nations[\"rugged\"], \n",
    "                   non_african_nations[\"mu_perc_5\"],\n",
    "                   non_african_nations[\"mu_perc_95\"],\n",
    "                   alpha=0.5)\n",
    "ax[0].plot(non_african_nations[\"rugged\"], \n",
    "           non_african_nations[\"true_gdp\"],\n",
    "           \"o\")\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"Non African Nations\")\n",
    "idx = np.argsort(african_nations[\"rugged\"])\n",
    "ax[1].plot(african_nations[\"rugged\"], \n",
    "           african_nations[\"mu_mean\"])\n",
    "ax[1].fill_between(african_nations[\"rugged\"],\n",
    "                   african_nations[\"mu_perc_5\"],\n",
    "                   african_nations[\"mu_perc_95\"],\n",
    "                   alpha=0.5)\n",
    "ax[1].plot(african_nations[\"rugged\"], \n",
    "           african_nations[\"true_gdp\"],\n",
    "           \"o\")\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"African Nations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the uncertainty in our estimate of the regression line. Note that for lower values of ruggedness there are many more data points, and as such, lesser wiggle room for the line of best fit. This is reflected in the 90% CI around the mean. We can also see that most of the data points actually lie outside the 90% CI, and this is expected because we have not plotted the outcome variable which will be affected by `sigma`! Let us do so next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "fig.suptitle(\"Posterior predictive distribution with 90% CI\", fontsize=16)\n",
    "ax[0].plot(non_african_nations[\"rugged\"], \n",
    "           non_african_nations[\"y_mean\"])\n",
    "ax[0].fill_between(non_african_nations[\"rugged\"], \n",
    "                   non_african_nations[\"y_perc_5\"],\n",
    "                   non_african_nations[\"y_perc_95\"],\n",
    "                   alpha=0.5)\n",
    "ax[0].plot(non_african_nations[\"rugged\"], \n",
    "           non_african_nations[\"true_gdp\"],\n",
    "           \"o\")\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"Non African Nations\")\n",
    "idx = np.argsort(african_nations[\"rugged\"])\n",
    "ax[1].plot(african_nations[\"rugged\"], \n",
    "           african_nations[\"y_mean\"])\n",
    "ax[1].fill_between(african_nations[\"rugged\"],\n",
    "                   african_nations[\"y_perc_5\"],\n",
    "                   african_nations[\"y_perc_95\"],\n",
    "                   alpha=0.5)\n",
    "ax[1].plot(african_nations[\"rugged\"], \n",
    "           african_nations[\"true_gdp\"],\n",
    "           \"o\")\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"African Nations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the outcome from our model and the 90% CI accounts for the majority of the data points that we observe in practice. It is a good idea to do such posterior predictive checks to see if our model gives valid predictions.\n",
    "\n",
    "Let us plot the distribution of the slope of the log GDP given terrain ruggedness for nations within and outside Africa. As can be seen below, the probability mass for African nations is largely concentrated in the positive region and vice-versa for other nations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sites = ['module$$$linear.weight']\n",
    "svi_empirical = EmpiricalMarginal(posterior, sites=sites)\\\n",
    "                    .get_samples_and_weights()[0].squeeze(1).squeeze(1).detach().numpy()\n",
    "gamma_within_africa = svi_empirical[:, 0] + svi_empirical[:, 1]\n",
    "# gamma_outside_africa = svi_empirical[:, 1]\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.distplot(gamma_within_africa, kde_kws={\"label\": \"African nations\"},)\n",
    "# sns.distplot(gamma_outside_africa, kde_kws={\"label\": \"Non-African nations\"})\n",
    "fig.suptitle(\"Density of Slope : log(GDP) vs. Terrain Ruggedness\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll look at how to write guides for variational inference as well as compare the results with inference via HMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the full code on [Github](https://github.com/uber/pyro/blob/dev/examples/bayesian_regression.py)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
